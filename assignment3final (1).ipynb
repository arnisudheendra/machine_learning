{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                                # TASK-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "import operator\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SkLearn Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "     'this is the first document',\n",
    "     'this document is the second document',\n",
    "     'and this is the third one',\n",
    "     'is this the first document',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SKLEARN IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Vacabulory\n",
      "\n",
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "\n",
      "row-coulmn     TFIDF value\n",
      "  (0, 8)\t0.38408524091481483\n",
      "  (0, 3)\t0.38408524091481483\n",
      "  (0, 6)\t0.38408524091481483\n",
      "  (0, 2)\t0.5802858236844359\n",
      "  (0, 1)\t0.46979138557992045\n",
      "\n",
      "         SHAPE of the matrix\n",
      "\n",
      "(4, 9)\n",
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n",
      "the IDF values are: [1.91629073 1.22314355 1.51082562 1.         1.91629073 1.91629073\n",
      " 1.         1.91629073 1.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vect=TfidfVectorizer() #calling tfidf vectorizer and storing it in vect\n",
    "final=vect.fit_transform(corpus) #CSR matrix is present here\n",
    "print('         {}'.format('Vacabulory'))\n",
    "print('')\n",
    "print(vect.get_feature_names()) #prints set of unique words/vocabulary\n",
    "print('')\n",
    "print(type(final)) #type of the corpus\n",
    "print('')\n",
    "print('{}     {}'.format('row-coulmn','TFIDF value'))\n",
    "print(final[0])\n",
    "print('')\n",
    "print('         {}'.format('SHAPE of the matrix'))\n",
    "print('')\n",
    "print(final.shape)\n",
    "\n",
    "\n",
    "print(final[0].toarray())\n",
    "print('the IDF values are:',vect.idf_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Custom Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 0, 'document': 1, 'first': 2, 'is': 3, 'one': 4, 'second': 5, 'the': 6, 'third': 7, 'this': 8}\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def fit(corps):\n",
    "    words=set()\n",
    "    if isinstance(corpus,(list)):  #check if the datatypes match\n",
    "        for row in corpus:         #for all the rows in the dataset\n",
    "            for word in row.split(): #split the sentence into  individual words based when ever 'empty space' is encountered.\n",
    "                if len(word) <2:  #ignore 'commas' as dataset is a list. by default is also considers 'comma' as a word.\n",
    "                    continue #else continue\n",
    "                words.add(word)  #appendthe word to the list of words\n",
    "        words=sorted(list(words)) #sort the words in alphabetical order\n",
    "        vocab={j:i for i,j in enumerate(words)} #return the unique words in the set\n",
    "        return vocab\n",
    "    else:\n",
    "        print(\"please pass a list of sentenses\")\n",
    "\n",
    "voc=fit(corpus)\n",
    "print(voc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this'])\n"
     ]
    }
   ],
   "source": [
    "print(voc.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for the above keys we have to find how many times each key occured in the whole corpus(inverse document frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPUTE IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the IDF values are [1.916290731874155, 1.2231435513142097, 1.5108256237659907, 1.0, 1.916290731874155, 1.916290731874155, 1.0, 1.916290731874155, 1.0]"
     ]
    }
   ],
   "source": [
    "idf=[]\n",
    "total=len(corpus) #total number of documents in the corpus\n",
    "def idfreq(corp,vocabulory): #calculates the freq of each unique word in the corpus\n",
    "    for i in unique.keys(): #for each unique word\n",
    "        count=0\n",
    "        for row in corpus:  #for each sentence in the corpus\n",
    "            if i in row.split(' '):\n",
    "                count+=1\n",
    "        idf.append(math.log((total+1)/(1+count))+1) #total=total nimber of documents ; count=number of docs containing \n",
    "    return idf\n",
    "x=idfreq(corpus,voc)\n",
    "print('the IDF values are',x,end='')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSFORM FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 9)\n",
      "  (0, 1)\t0.4697913855799205\n",
      "  (0, 2)\t0.580285823684436\n",
      "  (0, 3)\t0.3840852409148149\n",
      "  (0, 6)\t0.3840852409148149\n",
      "  (0, 8)\t0.3840852409148149\n",
      "\n",
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "def transform(dataset,vocab,idf):\n",
    "    rows = []\n",
    "    columns = []\n",
    "    values = []\n",
    "    if isinstance(corpus, (list,)):\n",
    "        for idx, row in enumerate(tqdm(corpus)): # for each document in the dataset\n",
    "            TF_cnt = len(row.split(\" \"))\n",
    "            # it will return a dict type object where key is the word and values is its frequency, {word:frequency}\n",
    "            word_freq = dict(Counter(row.split()))\n",
    "            # for every unique word in the document\n",
    "            for word, freq in word_freq.items():  # for each unique word in the review.                \n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                # we will check if its there in the vocabulary that we build in fit() function\n",
    "                # dict.get() function will return the values, if the key doesn't exits it will return -1\n",
    "                col_index = unique.get(word, -1) # retreving the dimension number of a word\n",
    "                # if the word exists\n",
    "                if col_index !=-1:\n",
    "                    # we are storing the index of the document\n",
    "                    rows.append(idx)\n",
    "                    # we are storing the dimensions of the word\n",
    "                    columns.append(col_index)\n",
    "                    # we are storing the frequency of the word\n",
    "                    # we are appending tfidf values into the values\n",
    "                    values.append((freq/TF_cnt)*idf[unique[word]])\n",
    "        return csr_matrix((values, (rows,columns)), shape=(len(corpus),len(voc)))\n",
    "    else:\n",
    "        print(\"you need to pass list of strings\")\n",
    "\n",
    "tnf = transform(corpus,voc,x)\n",
    "\n",
    "print(tnf.shape)\n",
    "\n",
    "res = normalize(tnf, norm='l2', axis=1)\n",
    "print(res[0])\n",
    "print('')\n",
    "print(res[0].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in corpus =  746\n",
      "\n",
      " unique words:\n",
      " {'aailiyah': 0, 'abandoned': 1, 'abroad': 2, 'abstruse': 3, 'academy': 4, 'accents': 5, 'accessible': 6, 'acclaimed': 7, 'accolades': 8, 'accurate': 9, 'accurately': 10, 'achille': 11, 'ackerman': 12, 'actions': 13, 'adams': 14, 'add': 15, 'added': 16, 'admins': 17, 'admiration': 18, 'admitted': 19, 'adrift': 20, 'adventure': 21, 'aesthetically': 22, 'affected': 23, 'affleck': 24, 'afternoon': 25, 'aged': 26, 'ages': 27, 'agree': 28, 'agreed': 29, 'aimless': 30, 'aired': 31, 'akasha': 32, 'akin': 33, 'alert': 34, 'alike': 35, 'allison': 36, 'allow': 37, 'allowing': 38, 'alongside': 39, 'amateurish': 40, 'amaze': 41, 'amazed': 42, 'amazingly': 43, 'amusing': 44, 'amust': 45, 'anatomist': 46, 'angel': 47, 'angela': 48, 'angelina': 49}\n",
      "--------------------------------------------------------------\n",
      "\n",
      " IDF values:\n",
      " [6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872]\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 746/746 [00:00<00:00, 82927.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(746, 50)\n",
      "\n",
      "\n",
      "\n",
      " result \n",
      "   (0, 30)\t1.0\n",
      "--------------------------------------------------------------\n",
      "\n",
      " result to  array:\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]]\n",
      "--------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.sparse import csr_matrix\n",
    "import pickle\n",
    "\n",
    "with open('cleaned_strings', 'rb') as f:\n",
    "    corpus1 = pickle.load(f)\n",
    "    \n",
    "# printing the length of the corpus loaded\n",
    "print(\"Number of documents in corpus = \",len(corpus1))\n",
    "#Fit function\n",
    "def fit(dataset):\n",
    "    unique_words = set()\n",
    "    if isinstance(dataset, (list,)):\n",
    "        for row in dataset:\n",
    "            for word in row.split(\" \"):\n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                unique_words.add(word)\n",
    "        unique_words = sorted(list(unique_words))\n",
    "        vocab = {j:i for i,j in enumerate(unique_words)}\n",
    "        return vocab\n",
    "    else:\n",
    "        print(\"you need to pass list of sentence\")\n",
    "#Computing IDF\n",
    "def computeIDF(dataset, fv):\n",
    "    idf=[]\n",
    "    Total = len(dataset)\n",
    "    for i in fv.keys():\n",
    "        count = 0\n",
    "        for row in dataset:\n",
    "            if i in row.split(\" \"):\n",
    "                count+=1\n",
    "        idf.append(math.log((Total+1)/(1+count))+1)\n",
    "    return idf\n",
    "#Transform function\n",
    "def transform(dataset,vocab,idf):\n",
    "    rows = []\n",
    "    columns = []\n",
    "    values = []\n",
    "    if isinstance(dataset, (list,)):\n",
    "        for idx, row in enumerate(tqdm(dataset)): # for each document in the dataset\n",
    "            TF_cnt = len(row.split(\" \"))\n",
    "            word_freq = dict(Counter(row.split())) #frequency of each word\n",
    "            # for every unique word in the document\n",
    "            for word, freq in word_freq.items():  # for each unique word in the review.                \n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                # dict.get() function will return the values, if the key doesn't exits it will return -1\n",
    "                col_index = vocab.get(word, -1) \n",
    "                if col_index !=-1:\n",
    "                    rows.append(idx)\n",
    "                    columns.append(col_index)\n",
    "                    values.append((freq/TF_cnt)*idf[vocab[word]])\n",
    "        return csr_matrix((values, (rows,columns)), shape=(len(dataset),len(vocab)))\n",
    "    else:\n",
    "        print(\"you need to pass list of strings\")\n",
    "\n",
    "voc1=fit(corpus1)\n",
    "idf1=computeIDF(corpus1,voc1)\n",
    "for i in voc1.keys():\n",
    "    voc1[i]=idf1[voc1[i]]\n",
    "#https://stackoverflow.com/questions/20577840/python-dictionary-sorting-in-descending-order-based-on-values\n",
    "voc1 = sorted(voc1.items(), key=lambda kv: kv[1],reverse=True)\n",
    "voc1=voc1[:50]\n",
    "#https://stackoverflow.com/questions/10777271/python-using-enumerate-inside-list-comprehension\n",
    "voc1={j[0]:i for i,j in enumerate(voc1)}\n",
    "idf1=sorted(idf1,reverse=True)\n",
    "idf1=idf1[:50]\n",
    "print(\"\\n unique words:\\n\",voc1)\n",
    "print(\"--------------------------------------------------------------\")\n",
    "print(\"\\n IDF values:\\n\",idf1)\n",
    "print(\"--------------------------------------------------------------\")\n",
    "tnf1=transform(corpus1,voc1,idf1)\n",
    "print(tnf1.shape)\n",
    "print(\"\\n\")\n",
    "res=normalize(tnf1,norm='l2',axis=1)\n",
    "print(\"\\n result \\n\",res[0])\n",
    "print(\"--------------------------------------------------------------\")\n",
    "print(\"\\n result to  array:\\n\",res[0].toarray())\n",
    "print(\"--------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
